{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"resnet.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1q3P7svmc8tOCS1IjH-Rh_aSg_6unlw2N","authorship_tag":"ABX9TyPetc9Zv+B0v3MX8SPH0VgQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"dY_3eSSpeflU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"063c23a6-2a35-4485-edf5-03a0b96c0b56","executionInfo":{"status":"error","timestamp":1589698986687,"user_tz":-480,"elapsed":4159184,"user":{"displayName":"zqstrive","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggr24K_aoT3UYoqlbf6c5wwDOD052P8YoqRDHGp-A=s64","userId":"15827624022674906427"}}},"source":["# lib that need\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","import os\n","import scipy.io as sio\n","from torch.autograd import Variable\n","from tqdm import tqdm\n","\n","from PIL import Image\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","# says it can accelerate the model\n","torch.backends.cudnn.benchmark = True\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')\n","\n","# data preprocess  -> 224X224 like resnet paper use\n","transform = transforms.Compose(\n","    [transforms.Resize(1024),\n","     transforms.CenterCrop(224),\n","     transforms.ToTensor(),\n","     transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])]\n",")\n","\n","class MyDataset(Dataset):\n","    def __init__(self,root,transform=None):\n","        super(MyDataset,self).__init__()\n","        self.root = root\n","        self.img_list = os.listdir(root)\n","        self.transform = transform\n","\n","    def __getitem__(self,index):\n","        img_name = os.path.join(self.root,self.img_list[index])\n","        if self.img_list[index][0] == 'h':\n","            label = 1\n","        else:\n","            label = 0\n","        img = Image.open(img_name)\n","        # img_tensor = transforms.ToTensor(img)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        return img,label\n","\n","    def __len__(self):\n","        return len(self.img_list)\n","\n","# dataset and dataloader\n","train_set = MyDataset(root='/content/drive/My Drive/QML/cv/image_classification/train set',transform=transform)\n","test_set = MyDataset(root='/content/drive/My Drive/QML/cv/image_classification/test set',transform=transform)\n","\n","batch_size = 10\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True,num_workers=4)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False,num_workers=4)\n","\n","# for (inputs, labels) in train_loader:\n","#     print(inputs.shape)\n","#     print(labels)\n","\n","\n","\n","# flatten the output to  full-connection\n","def flatten(x):\n","    N = x.shape[0]  # x -> N,C,H,W\n","    return x.view(N,-1)\n","class Flatten(nn.Module):\n","    def forward(self,x):\n","        return flatten(x)\n","\n","# classical convolution network\n","class LeNet(nn.Module):\n","    def __init__(self):\n","        super(LeNet, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 6, 5),\n","            nn.BatchNorm2d(6),\n","            nn.ReLU(inplace=True),  # reduce memory\n","            nn.MaxPool2d(2, 2)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(6, 16, 5),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, 2)\n","        )\n","        self.fc = nn.Sequential(\n","            Flatten(),\n","            nn.Linear(16 * 5 * 5, 120),\n","            nn.BatchNorm1d(120),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(120, 84),\n","            nn.BatchNorm1d(84),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(84, 10)\n","        )\n","\n","    def forward(self, x):\n","        x_conv1 = self.conv1(x)\n","        x_conv2 = self.conv2(x_conv1)\n","        # x_conv2 = x_conv2.view(-1, 16 * 5 * 5)\n","        x_fc = self.fc(x_conv2)\n","        return x_fc\n","\n","# residual block\n","class Bottleneck(nn.Module):\n","    def __init__(self, in_channels, stride=1, first_layer=False,down_sampling=False,next_layer=False,groups=1,depth=1):\n","        \"\"\"\n","\n","        :param in_channels: in_channels\n","        :param stride:stride of 3x3 conv\n","        :param first_layer:conv with no down_sampling\n","        :param down_sampling:down_sampling\n","        :param next_layer:different block of conv\n","        :param groups:3x3 conv groups\n","        :param depth:residual block depth\n","        \"\"\"\n","        super(Bottleneck, self).__init__()\n","        change_channels = 4*in_channels\n","        self.downsampling = down_sampling\n","        self.first_layer = first_layer\n","\n","        base_channels = 64      # base-channels\n","        if groups == 1:         # resnet\n","            width_channels = in_channels\n","            down_sampling_channels = 2*in_channels\n","        elif groups == 32:      # resnext\n","            width_channels = np.int(in_channels/base_channels)*depth*groups\n","            down_sampling_channels = width_channels\n","        if first_layer:\n","            self.conv1 = nn.Sequential(\n","                nn.Conv2d(in_channels,width_channels , kernel_size=1),\n","                nn.BatchNorm2d(width_channels),\n","                nn.ReLU(inplace=True)   # in_place to reduce memory\n","            )\n","        elif next_layer:\n","            self.conv1 = nn.Sequential(\n","                nn.Conv2d(down_sampling_channels, width_channels, kernel_size=1),\n","                nn.BatchNorm2d(width_channels),\n","                nn.ReLU(inplace=True)\n","            )\n","        else:\n","            self.conv1 = nn.Sequential(\n","                nn.Conv2d(change_channels, width_channels, kernel_size=1),  # the last layer of residual block in the conv block\n","                nn.BatchNorm2d(width_channels),                                 #                -> the first layer of next residual block\n","                nn.ReLU(inplace=True)\n","            )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(width_channels,width_channels,kernel_size=3,stride=stride,padding=1,groups=groups),\n","            nn.BatchNorm2d(width_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.conv3 = nn.Sequential(\n","                nn.Conv2d(width_channels,change_channels,kernel_size=1),   # change_channels are input_channels->1X1 conv2_nums\n","                nn.BatchNorm2d(change_channels)\n","            )\n","        if self.downsampling:   # if down_sampling  define layer stride = 2\n","            self.downsampling_layer = nn.Sequential(\n","                nn.Conv2d(down_sampling_channels,change_channels,1,stride=2),\n","                nn.BatchNorm2d(change_channels)\n","            )\n","        # conv2 first add 1x1_conv to let dim same\n","        if first_layer:\n","            self.downsampling_layer = nn.Sequential(\n","                nn.Conv2d(in_channels,change_channels,1,stride=1),\n","                nn.BatchNorm2d(change_channels)\n","            )\n","    def forward(self,x):\n","        identity = x\n","        x_conv1 = self.conv1(x)\n","        x_conv2 = self.conv2(x_conv1)\n","        x_conv3 = self.conv3(x_conv2)\n","        if self.downsampling or self.first_layer:   # if down_sampling x should go through down_sampling layer(1x1 conv ) to let dim same\n","            identity = self.downsampling_layer(x)\n","        return F.relu(x_conv3+identity)\n","\n","# conv_block\n","def resnet_block(in_channels,num_block,down_sampling=False,change=False,groups=1,depth=1):\n","    \"\"\"\n","\n","    :param in_channels: in_channels\n","    :param num_block: num of block\n","    :param down_sampling: down_sampling or not\n","    :param change:stride from 1->2 residual block 3x3 conv\n","    :param groups:groups\n","    :param depth: depth\n","    :return:\n","    \"\"\"\n","    conv_blk = []\n","    for i in range(num_block):\n","        if i == 0 :     # first layer of block\n","            if change:\n","                conv_blk.append(Bottleneck(in_channels,first_layer=False,stride=2,down_sampling=down_sampling,next_layer=True,groups=groups,depth=depth))\n","            else:\n","                conv_blk.append(\n","                    Bottleneck(in_channels, first_layer=True, stride=1, down_sampling=down_sampling,groups=groups,depth=depth))     # conv2 block first layer\n","        else:\n","            conv_blk.append(Bottleneck(in_channels,first_layer=False,stride=1,down_sampling=False,groups=groups,depth=depth))       # the left layer\n","\n","    return nn.Sequential(*conv_blk)\n","\n","class ResNet_50(nn.Module):\n","    def __init__(self):\n","        super(ResNet_50, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=7, stride=2,padding=3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.MaxPool2d(3,2,padding=1),\n","            nn.Sequential(resnet_block(64,3,down_sampling=False))\n","        )\n","        self.conv3 = resnet_block(128,4,down_sampling=True,change=True)\n","        self.conv4 = resnet_block(256,6,down_sampling=True,change=True)\n","        self.conv5 = resnet_block(512,3,down_sampling=True,change=True)\n","        self.fc = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((1, 1)),   # global avgerage pool\n","            Flatten(),\n","            nn.Linear(512 * 4, 2),\n","            nn.Softmax(dim=1)\n","        )\n","    def forward(self,x):\n","        x_conv1 = self.conv1(x)\n","        x_conv2 = self.conv2(x_conv1)\n","        x_conv3 = self.conv3(x_conv2)\n","        x_conv4 = self.conv4(x_conv3)\n","        x_conv5 = self.conv5(x_conv4)\n","        output = self.fc(x_conv5)\n","        return output\n","\n","resnet_50 = ResNet_50().to(device)\n","# print(resnet_50)\n","\n","lr, num_epochs = 0.001, 1000\n","\n","resnet_50_loss_list = []\n","\n","resnet_50_acc_list = []\n","#\n","\n","\n","# test each conv output shape\n","def test_net(net):\n","    net_tn = net\n","    X = torch.rand((1, 3, 224, 224)).to(device)\n","    for name, layer in net_tn.named_children():\n","        X = layer(X)\n","        print(name, ' output shape:\\t', X.shape)\n","# save net,loss_list and acc_list\n","def save_model(net,loss_list,acc_list):\n","    net = net.cpu()\n","    loss_list_sm = loss_list\n","    acc_list_sm = acc_list\n","    sio.savemat('/content/drive/My Drive/QML/cv/image_classification/loss.mat', {'loss': loss_list_sm})\n","    sio.savemat('/content/drive/My Drive/QML/cv/image_classification/acc.mat', {'acc': acc_list_sm})\n","    print(\"model saving...\")\n","    torch.save(net, '/content/drive/My Drive/QML/cv/image_classification/{0}.pth'.format(net.__class__.__name__))\n","\n","def train_and_test(net,train_loader,test_loader, loss_list,lr,num_epochs,acc_list):\n","\n","    train_loader = train_loader\n","    test_loader = test_loader\n","    net = net.to(device)\n","    loss_list_ep = loss_list\n","    lr = lr\n","    num_epochs = num_epochs\n","    acc_list_ep = acc_list\n","    optimizer = optim.Adam(net.parameters(), lr=lr)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    start_epoch = -1        # checkpoint start_epoch\n","    # load checkpoint\n","    model_file = '/content/drive/My Drive/QML/cv/image_classification/checkpoint.pth'\n","    if os.path.isfile(model_file):\n","        checkpoint = torch.load(model_file)\n","        acc_cp = checkpoint['acc']\n","        start_epoch = checkpoint['epoch']\n","        net.load_state_dict(checkpoint['model'])\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","        loss_list_ep = checkpoint['loss_list']\n","        acc_list_cp = checkpoint['acc_list']\n","        print('Load checkpoint at epoch %d | acc %.2f' % (start_epoch, acc_cp))\n","\n","\n","    for epoch in range(num_epochs):\n","        if epoch <= start_epoch:\n","            continue\n","        start_time = time.time()\n","        train_loss = 0.0\n","        test_acc = 0.0\n","        train_loss_n = 0\n","        test_acc_n = 0\n","        for iter,(inputs, labels) in enumerate(train_loader):     # train\n","\n","            # inputs = inputs.to(device)\n","            inputs_img = Variable(inputs).to(device)\n","            # labels = labels.to(device)\n","            labels = Variable(labels).to(device)\n","\n","            labels_predict = net(inputs_img)\n","\n","            loss = loss_func(labels_predict, labels)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if (iter+1)%5 == 0:\n","                loss_list_ep.append(loss.item())\n","                # print('train_loss %.4f '%(loss.item()))\n","            train_loss += loss.cpu().item()\n","            train_loss_n += 1\n","\n","            # del intermediate variables/tensors to reduce memory\n","            del inputs_img\n","            del labels\n","            del labels_predict\n","            del loss\n","            # maybe helpful\n","            torch.cuda.empty_cache()\n","        train_loss /= train_loss_n\n","        # print('epoch %d | train_loss %.4f'%(epoch,train_loss))\n","        loss_list_ep.append(train_loss)\n","\n","        for inputs, labels in test_loader:        # test\n","            net.eval()  # test mode to prevent BN influence\n","            with torch.no_grad():\n","                inputs_img = Variable(inputs).to(device)\n","                labels = Variable(labels).to(device)\n","                labels_predict = net(inputs_img)\n","                test_acc += torch.sum(labels_predict.cpu().argmax(dim=1) == labels.cpu()).numpy()\n","                test_acc_n += labels.shape[0]\n","                # print('test_acc %.2f' % (test_acc / test_acc_n))\n","            net.train()  # train mode\n","        test_acc /= test_acc_n\n","        acc_list_ep.append(test_acc)\n","        print('epoch %d | train_loss %.4f | test_acc %.2f | time %.2f sec'\n","              % (epoch, train_loss, test_acc , time.time() - start_time))\n","        \n","        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'max',factor=0.1,patience=3,verbose=True)\n","        # scheduler.step(test_acc)\n","        # print(optimizer.state_dict()['param_groups'][0]['lr'])\n","\n","        # save checkpoint\n","        checkpoint = {\n","            'acc': test_acc,\n","            'epoch': epoch,\n","            'model': net.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'loss_list': loss_list_ep,\n","            'acc_list' : acc_list_ep\n","        }\n","        model_file = '/content/drive/My Drive/QML/cv/image_classification/checkpoint.pth'\n","        torch.save(checkpoint, model_file)\n","        if test_acc>=0.90:      # reach acc to save model\n","            save_model(net,loss_list_ep,acc_list_ep)\n","\n","train_and_test(resnet_50,train_loader,test_loader,resnet_50_loss_list,lr,num_epochs,resnet_50_acc_list)\n","\n","\n","## cost function figure\n","# data_loss = sio.loadmat('/content/drive/My Drive/homework/cnn_model/loss.mat')\n","# loss_list = data_loss['loss']\n","# # print(loss_list)\n","# # print(np.nonzero(loss_list))\n","# loss_plt = []\n","# print(loss_list.shape[1])\n","# for i in range(loss_list.shape[1]):\n","#     if loss_list[0][i] > 0 and (i+1)%1000==0:\n","#         loss_plt.append(loss_list[0][i])\n","# list_iter = range(len(loss_plt))\n","# def draw(num_iter,loss_list):\n","#     plt.plot(num_iter,loss_list,'-b',label='loss')\n","#     plt.xlabel('epoch')\n","#     plt.ylabel('loss')\n","#     plt.title('loss')\n","#     plt.savefig('/content/drive/My Drive/homework/cnn_model/loss.eps',format='eps')\n","# draw(list_iter,loss_plt)\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["epoch 0 | train_loss 0.6922 | test_acc 0.50 | time 5.81 sec\n","epoch 1 | train_loss 0.7758 | test_acc 0.50 | time 4.39 sec\n","epoch 2 | train_loss 0.8038 | test_acc 0.60 | time 4.30 sec\n","epoch 3 | train_loss 0.7863 | test_acc 0.53 | time 4.33 sec\n","epoch 4 | train_loss 0.7235 | test_acc 0.57 | time 4.40 sec\n","epoch 5 | train_loss 0.6775 | test_acc 0.63 | time 4.36 sec\n","epoch 6 | train_loss 0.6490 | test_acc 0.50 | time 4.32 sec\n","epoch 7 | train_loss 0.6783 | test_acc 0.57 | time 4.44 sec\n","epoch 8 | train_loss 0.8776 | test_acc 0.47 | time 4.46 sec\n","epoch 9 | train_loss 0.9058 | test_acc 0.53 | time 4.37 sec\n","epoch 10 | train_loss 0.8645 | test_acc 0.40 | time 4.33 sec\n","epoch 11 | train_loss 0.8825 | test_acc 0.47 | time 4.39 sec\n","epoch 12 | train_loss 0.8381 | test_acc 0.43 | time 4.42 sec\n","epoch 13 | train_loss 0.7754 | test_acc 0.53 | time 4.35 sec\n","epoch 14 | train_loss 0.8527 | test_acc 0.50 | time 4.20 sec\n","epoch 15 | train_loss 0.8174 | test_acc 0.50 | time 4.30 sec\n","epoch 16 | train_loss 0.7569 | test_acc 0.57 | time 4.36 sec\n","epoch 17 | train_loss 0.7488 | test_acc 0.57 | time 4.38 sec\n","epoch 18 | train_loss 0.7634 | test_acc 0.43 | time 4.24 sec\n","epoch 19 | train_loss 0.7083 | test_acc 0.67 | time 4.44 sec\n","epoch 20 | train_loss 0.6588 | test_acc 0.70 | time 9.61 sec\n","epoch 21 | train_loss 0.6742 | test_acc 0.57 | time 4.31 sec\n","epoch 22 | train_loss 0.6400 | test_acc 0.67 | time 4.38 sec\n","epoch 23 | train_loss 0.6802 | test_acc 0.67 | time 4.41 sec\n","epoch 24 | train_loss 0.7364 | test_acc 0.63 | time 4.34 sec\n","epoch 25 | train_loss 0.7376 | test_acc 0.63 | time 5.64 sec\n","epoch 26 | train_loss 0.6792 | test_acc 0.73 | time 4.52 sec\n","epoch 27 | train_loss 0.6313 | test_acc 0.70 | time 4.46 sec\n","epoch 28 | train_loss 0.6789 | test_acc 0.67 | time 4.63 sec\n","epoch 29 | train_loss 0.7273 | test_acc 0.70 | time 4.39 sec\n","epoch 30 | train_loss 0.7902 | test_acc 0.63 | time 4.43 sec\n","epoch 31 | train_loss 0.7258 | test_acc 0.70 | time 4.28 sec\n","epoch 32 | train_loss 0.6640 | test_acc 0.67 | time 4.48 sec\n","epoch 33 | train_loss 0.6348 | test_acc 0.47 | time 4.32 sec\n","epoch 34 | train_loss 0.6918 | test_acc 0.50 | time 4.76 sec\n","epoch 35 | train_loss 0.6137 | test_acc 0.53 | time 4.24 sec\n","epoch 36 | train_loss 0.6968 | test_acc 0.60 | time 4.45 sec\n","epoch 37 | train_loss 0.7110 | test_acc 0.60 | time 4.36 sec\n","epoch 38 | train_loss 0.6257 | test_acc 0.67 | time 4.38 sec\n","epoch 39 | train_loss 0.6619 | test_acc 0.60 | time 4.25 sec\n","epoch 40 | train_loss 0.6568 | test_acc 0.60 | time 4.73 sec\n","epoch 41 | train_loss 0.7989 | test_acc 0.60 | time 4.33 sec\n","epoch 42 | train_loss 0.8129 | test_acc 0.60 | time 4.26 sec\n","epoch 43 | train_loss 0.7480 | test_acc 0.67 | time 4.94 sec\n","epoch 44 | train_loss 0.6807 | test_acc 0.63 | time 5.90 sec\n","epoch 45 | train_loss 0.6875 | test_acc 0.57 | time 4.43 sec\n","epoch 46 | train_loss 0.6655 | test_acc 0.60 | time 4.37 sec\n","epoch 47 | train_loss 0.5645 | test_acc 0.43 | time 4.18 sec\n","epoch 48 | train_loss 0.6986 | test_acc 0.50 | time 4.28 sec\n","epoch 49 | train_loss 0.7428 | test_acc 0.43 | time 4.25 sec\n","epoch 50 | train_loss 0.8640 | test_acc 0.47 | time 4.24 sec\n","epoch 51 | train_loss 0.8774 | test_acc 0.67 | time 4.37 sec\n","epoch 52 | train_loss 0.7482 | test_acc 0.50 | time 4.46 sec\n","epoch 53 | train_loss 0.7016 | test_acc 0.60 | time 4.36 sec\n","epoch 54 | train_loss 0.6875 | test_acc 0.60 | time 4.28 sec\n","epoch 55 | train_loss 0.6690 | test_acc 0.73 | time 4.39 sec\n","epoch 56 | train_loss 0.6633 | test_acc 0.73 | time 4.79 sec\n","epoch 57 | train_loss 0.6713 | test_acc 0.67 | time 4.20 sec\n","epoch 58 | train_loss 0.6985 | test_acc 0.53 | time 4.37 sec\n","epoch 59 | train_loss 0.6468 | test_acc 0.67 | time 4.42 sec\n","epoch 60 | train_loss 0.6628 | test_acc 0.73 | time 4.33 sec\n","epoch 61 | train_loss 0.6165 | test_acc 0.70 | time 4.34 sec\n","epoch 62 | train_loss 0.6762 | test_acc 0.77 | time 4.26 sec\n","epoch 63 | train_loss 0.6292 | test_acc 0.77 | time 4.37 sec\n","epoch 64 | train_loss 0.6331 | test_acc 0.63 | time 4.27 sec\n","epoch 65 | train_loss 0.6854 | test_acc 0.63 | time 4.46 sec\n","epoch 66 | train_loss 0.5563 | test_acc 0.63 | time 4.31 sec\n","epoch 67 | train_loss 0.5635 | test_acc 0.77 | time 10.14 sec\n","epoch 68 | train_loss 0.5881 | test_acc 0.77 | time 4.29 sec\n","epoch 69 | train_loss 0.5768 | test_acc 0.80 | time 4.26 sec\n","epoch 70 | train_loss 0.5859 | test_acc 0.77 | time 4.70 sec\n","epoch 71 | train_loss 0.5995 | test_acc 0.77 | time 5.83 sec\n","epoch 72 | train_loss 0.6212 | test_acc 0.77 | time 4.26 sec\n","epoch 73 | train_loss 0.6805 | test_acc 0.73 | time 4.23 sec\n","epoch 74 | train_loss 0.6253 | test_acc 0.73 | time 4.31 sec\n","epoch 75 | train_loss 0.6424 | test_acc 0.67 | time 4.51 sec\n","epoch 76 | train_loss 0.6700 | test_acc 0.67 | time 4.38 sec\n","epoch 77 | train_loss 0.7069 | test_acc 0.63 | time 4.42 sec\n","epoch 78 | train_loss 0.7254 | test_acc 0.63 | time 4.39 sec\n","epoch 79 | train_loss 0.7760 | test_acc 0.67 | time 4.21 sec\n","epoch 80 | train_loss 0.8040 | test_acc 0.70 | time 4.31 sec\n","epoch 81 | train_loss 0.6189 | test_acc 0.60 | time 4.32 sec\n","epoch 82 | train_loss 0.6022 | test_acc 0.53 | time 4.31 sec\n","epoch 83 | train_loss 0.7169 | test_acc 0.50 | time 4.24 sec\n","epoch 84 | train_loss 0.8284 | test_acc 0.50 | time 9.65 sec\n","epoch 85 | train_loss 0.8646 | test_acc 0.50 | time 4.21 sec\n","epoch 86 | train_loss 0.8121 | test_acc 0.50 | time 4.31 sec\n","epoch 87 | train_loss 0.8368 | test_acc 0.50 | time 4.15 sec\n","epoch 88 | train_loss 0.8025 | test_acc 0.50 | time 4.52 sec\n","epoch 89 | train_loss 0.8044 | test_acc 0.37 | time 9.55 sec\n","epoch 90 | train_loss 0.7586 | test_acc 0.50 | time 4.30 sec\n","epoch 91 | train_loss 0.7430 | test_acc 0.73 | time 4.15 sec\n","epoch 92 | train_loss 0.7182 | test_acc 0.70 | time 4.23 sec\n","epoch 93 | train_loss 0.7049 | test_acc 0.70 | time 4.47 sec\n","epoch 94 | train_loss 0.6728 | test_acc 0.63 | time 4.89 sec\n","epoch 95 | train_loss 0.6873 | test_acc 0.57 | time 4.27 sec\n","epoch 96 | train_loss 0.5950 | test_acc 0.67 | time 4.40 sec\n","epoch 97 | train_loss 0.6246 | test_acc 0.73 | time 4.33 sec\n","epoch 98 | train_loss 0.6283 | test_acc 0.73 | time 4.37 sec\n","epoch 99 | train_loss 0.7329 | test_acc 0.73 | time 6.49 sec\n","epoch 100 | train_loss 0.5775 | test_acc 0.67 | time 4.40 sec\n","epoch 101 | train_loss 0.5874 | test_acc 0.77 | time 4.30 sec\n","epoch 102 | train_loss 0.5667 | test_acc 0.80 | time 9.98 sec\n","epoch 103 | train_loss 0.6129 | test_acc 0.80 | time 4.31 sec\n","epoch 104 | train_loss 0.5494 | test_acc 0.73 | time 4.41 sec\n","epoch 105 | train_loss 0.5655 | test_acc 0.77 | time 4.39 sec\n","epoch 106 | train_loss 0.6199 | test_acc 0.70 | time 4.28 sec\n","epoch 107 | train_loss 0.5896 | test_acc 0.77 | time 4.40 sec\n","epoch 108 | train_loss 0.7483 | test_acc 0.70 | time 4.25 sec\n","epoch 109 | train_loss 0.8120 | test_acc 0.63 | time 4.44 sec\n","epoch 110 | train_loss 0.8138 | test_acc 0.77 | time 4.39 sec\n","epoch 111 | train_loss 0.7085 | test_acc 0.57 | time 10.16 sec\n","epoch 112 | train_loss 0.7602 | test_acc 0.53 | time 4.35 sec\n","epoch 113 | train_loss 0.8010 | test_acc 0.70 | time 4.18 sec\n","epoch 114 | train_loss 0.7693 | test_acc 0.57 | time 4.42 sec\n","epoch 115 | train_loss 0.7641 | test_acc 0.63 | time 4.34 sec\n","epoch 116 | train_loss 0.7093 | test_acc 0.67 | time 10.10 sec\n","epoch 117 | train_loss 0.6633 | test_acc 0.50 | time 4.33 sec\n","epoch 118 | train_loss 0.6368 | test_acc 0.43 | time 4.37 sec\n","epoch 119 | train_loss 0.6176 | test_acc 0.50 | time 4.35 sec\n","epoch 120 | train_loss 0.6518 | test_acc 0.53 | time 4.36 sec\n","epoch 121 | train_loss 0.6082 | test_acc 0.50 | time 9.51 sec\n","epoch 122 | train_loss 0.5974 | test_acc 0.50 | time 4.35 sec\n","epoch 123 | train_loss 0.6407 | test_acc 0.50 | time 4.28 sec\n","epoch 124 | train_loss 0.6050 | test_acc 0.57 | time 4.39 sec\n","epoch 125 | train_loss 0.6687 | test_acc 0.53 | time 4.74 sec\n","epoch 126 | train_loss 0.6862 | test_acc 0.63 | time 4.82 sec\n","epoch 127 | train_loss 0.6228 | test_acc 0.67 | time 4.33 sec\n","epoch 128 | train_loss 0.6445 | test_acc 0.67 | time 4.45 sec\n","epoch 129 | train_loss 0.6264 | test_acc 0.73 | time 4.66 sec\n","epoch 130 | train_loss 0.5915 | test_acc 0.77 | time 4.50 sec\n","epoch 131 | train_loss 0.6000 | test_acc 0.77 | time 4.36 sec\n","epoch 132 | train_loss 0.5855 | test_acc 0.73 | time 4.27 sec\n","epoch 133 | train_loss 0.5799 | test_acc 0.60 | time 4.29 sec\n","epoch 134 | train_loss 0.6024 | test_acc 0.57 | time 5.23 sec\n","epoch 135 | train_loss 0.5969 | test_acc 0.60 | time 4.41 sec\n","epoch 136 | train_loss 0.6014 | test_acc 0.60 | time 4.79 sec\n","epoch 137 | train_loss 0.6022 | test_acc 0.57 | time 4.47 sec\n","epoch 138 | train_loss 0.6043 | test_acc 0.60 | time 4.27 sec\n","epoch 139 | train_loss 0.6371 | test_acc 0.57 | time 4.27 sec\n","epoch 140 | train_loss 0.6248 | test_acc 0.63 | time 4.69 sec\n","epoch 141 | train_loss 0.6332 | test_acc 0.67 | time 4.65 sec\n","epoch 142 | train_loss 0.5607 | test_acc 0.60 | time 4.38 sec\n","epoch 143 | train_loss 0.6217 | test_acc 0.63 | time 4.33 sec\n","epoch 144 | train_loss 0.5801 | test_acc 0.70 | time 4.49 sec\n","epoch 145 | train_loss 0.5539 | test_acc 0.67 | time 4.33 sec\n","epoch 146 | train_loss 0.5624 | test_acc 0.57 | time 4.26 sec\n","epoch 147 | train_loss 0.5984 | test_acc 0.50 | time 4.36 sec\n","epoch 148 | train_loss 0.6882 | test_acc 0.47 | time 4.25 sec\n","epoch 149 | train_loss 0.6425 | test_acc 0.57 | time 4.32 sec\n","epoch 150 | train_loss 0.6367 | test_acc 0.57 | time 4.37 sec\n","epoch 151 | train_loss 0.6766 | test_acc 0.53 | time 4.31 sec\n","epoch 152 | train_loss 0.5762 | test_acc 0.73 | time 4.47 sec\n","epoch 153 | train_loss 0.6100 | test_acc 0.70 | time 4.29 sec\n","epoch 154 | train_loss 0.6240 | test_acc 0.70 | time 4.87 sec\n","epoch 155 | train_loss 0.5506 | test_acc 0.73 | time 4.21 sec\n","epoch 156 | train_loss 0.6133 | test_acc 0.70 | time 4.21 sec\n","epoch 157 | train_loss 0.5979 | test_acc 0.70 | time 4.34 sec\n","epoch 158 | train_loss 0.6025 | test_acc 0.73 | time 4.29 sec\n","epoch 159 | train_loss 0.5635 | test_acc 0.73 | time 9.95 sec\n","epoch 160 | train_loss 0.5682 | test_acc 0.57 | time 4.29 sec\n","epoch 161 | train_loss 0.5756 | test_acc 0.77 | time 4.34 sec\n","epoch 162 | train_loss 0.5519 | test_acc 0.80 | time 4.89 sec\n","epoch 163 | train_loss 0.5089 | test_acc 0.70 | time 4.24 sec\n","epoch 164 | train_loss 0.5628 | test_acc 0.70 | time 10.09 sec\n","epoch 165 | train_loss 0.5009 | test_acc 0.77 | time 4.37 sec\n","epoch 166 | train_loss 0.6010 | test_acc 0.73 | time 4.26 sec\n","epoch 167 | train_loss 0.5345 | test_acc 0.77 | time 4.15 sec\n","epoch 168 | train_loss 0.5816 | test_acc 0.60 | time 4.31 sec\n","epoch 169 | train_loss 0.5768 | test_acc 0.57 | time 4.34 sec\n","epoch 170 | train_loss 0.5507 | test_acc 0.53 | time 4.36 sec\n","epoch 171 | train_loss 0.6326 | test_acc 0.53 | time 4.37 sec\n","epoch 172 | train_loss 0.5133 | test_acc 0.53 | time 4.34 sec\n","epoch 173 | train_loss 0.5901 | test_acc 0.57 | time 7.97 sec\n","epoch 174 | train_loss 0.5989 | test_acc 0.67 | time 4.47 sec\n","epoch 175 | train_loss 0.6162 | test_acc 0.73 | time 4.40 sec\n","epoch 176 | train_loss 0.6529 | test_acc 0.67 | time 4.33 sec\n","epoch 177 | train_loss 0.6509 | test_acc 0.63 | time 4.25 sec\n","epoch 178 | train_loss 0.6528 | test_acc 0.70 | time 4.29 sec\n","epoch 179 | train_loss 0.5738 | test_acc 0.73 | time 4.28 sec\n","epoch 180 | train_loss 0.6291 | test_acc 0.63 | time 4.79 sec\n","epoch 181 | train_loss 0.6257 | test_acc 0.63 | time 4.33 sec\n","epoch 182 | train_loss 0.6354 | test_acc 0.63 | time 4.27 sec\n","epoch 183 | train_loss 0.6049 | test_acc 0.63 | time 4.42 sec\n","epoch 184 | train_loss 0.6507 | test_acc 0.60 | time 4.23 sec\n","epoch 185 | train_loss 0.5877 | test_acc 0.60 | time 4.46 sec\n","epoch 186 | train_loss 0.5968 | test_acc 0.63 | time 4.32 sec\n","epoch 187 | train_loss 0.5772 | test_acc 0.63 | time 4.98 sec\n","epoch 188 | train_loss 0.6017 | test_acc 0.63 | time 4.37 sec\n","epoch 189 | train_loss 0.5521 | test_acc 0.63 | time 4.47 sec\n","epoch 190 | train_loss 0.5959 | test_acc 0.63 | time 4.49 sec\n","epoch 191 | train_loss 0.5633 | test_acc 0.60 | time 4.39 sec\n","epoch 192 | train_loss 0.5627 | test_acc 0.63 | time 4.24 sec\n","epoch 193 | train_loss 0.5386 | test_acc 0.63 | time 4.26 sec\n","epoch 194 | train_loss 0.5563 | test_acc 0.67 | time 4.35 sec\n","epoch 195 | train_loss 0.5390 | test_acc 0.70 | time 4.23 sec\n","epoch 196 | train_loss 0.5547 | test_acc 0.67 | time 4.34 sec\n","epoch 197 | train_loss 0.5766 | test_acc 0.67 | time 4.31 sec\n","epoch 198 | train_loss 0.5862 | test_acc 0.67 | time 4.40 sec\n","epoch 199 | train_loss 0.6244 | test_acc 0.67 | time 4.24 sec\n","epoch 200 | train_loss 0.6403 | test_acc 0.67 | time 4.54 sec\n","epoch 201 | train_loss 0.6247 | test_acc 0.70 | time 4.70 sec\n","epoch 202 | train_loss 0.5789 | test_acc 0.73 | time 4.21 sec\n","epoch 203 | train_loss 0.5957 | test_acc 0.60 | time 4.31 sec\n","epoch 204 | train_loss 0.6807 | test_acc 0.40 | time 4.29 sec\n","epoch 205 | train_loss 0.8033 | test_acc 0.60 | time 4.39 sec\n","epoch 206 | train_loss 0.7846 | test_acc 0.73 | time 9.63 sec\n","epoch 207 | train_loss 0.7531 | test_acc 0.73 | time 4.20 sec\n","epoch 208 | train_loss 0.8007 | test_acc 0.73 | time 4.31 sec\n","epoch 209 | train_loss 0.7568 | test_acc 0.77 | time 4.32 sec\n","epoch 210 | train_loss 0.7538 | test_acc 0.60 | time 6.19 sec\n","epoch 211 | train_loss 0.7533 | test_acc 0.60 | time 4.17 sec\n","epoch 212 | train_loss 0.7877 | test_acc 0.67 | time 4.26 sec\n","epoch 213 | train_loss 0.7217 | test_acc 0.67 | time 4.24 sec\n","epoch 214 | train_loss 0.6382 | test_acc 0.73 | time 8.01 sec\n","epoch 215 | train_loss 0.6207 | test_acc 0.57 | time 4.45 sec\n","epoch 216 | train_loss 0.6168 | test_acc 0.53 | time 4.30 sec\n","epoch 217 | train_loss 0.5995 | test_acc 0.53 | time 4.23 sec\n","epoch 218 | train_loss 0.5920 | test_acc 0.53 | time 4.38 sec\n","epoch 219 | train_loss 0.6383 | test_acc 0.53 | time 4.26 sec\n","epoch 220 | train_loss 0.6401 | test_acc 0.63 | time 4.14 sec\n","epoch 221 | train_loss 0.5881 | test_acc 0.70 | time 4.18 sec\n","epoch 222 | train_loss 0.5669 | test_acc 0.70 | time 4.29 sec\n","epoch 223 | train_loss 0.5868 | test_acc 0.63 | time 5.67 sec\n","epoch 224 | train_loss 0.5853 | test_acc 0.67 | time 7.68 sec\n","epoch 225 | train_loss 0.5963 | test_acc 0.63 | time 4.35 sec\n","epoch 226 | train_loss 0.6003 | test_acc 0.53 | time 4.31 sec\n","epoch 227 | train_loss 0.6162 | test_acc 0.63 | time 4.92 sec\n","epoch 228 | train_loss 0.6109 | test_acc 0.70 | time 4.39 sec\n","epoch 229 | train_loss 0.6176 | test_acc 0.63 | time 9.94 sec\n","epoch 230 | train_loss 0.5819 | test_acc 0.63 | time 4.44 sec\n","epoch 231 | train_loss 0.6011 | test_acc 0.63 | time 4.32 sec\n","epoch 232 | train_loss 0.6711 | test_acc 0.60 | time 4.87 sec\n","epoch 233 | train_loss 0.6226 | test_acc 0.60 | time 4.41 sec\n","epoch 234 | train_loss 0.6143 | test_acc 0.67 | time 8.10 sec\n","epoch 235 | train_loss 0.5566 | test_acc 0.67 | time 4.37 sec\n","epoch 236 | train_loss 0.6677 | test_acc 0.67 | time 4.47 sec\n","epoch 237 | train_loss 0.6155 | test_acc 0.70 | time 5.95 sec\n","epoch 238 | train_loss 0.5743 | test_acc 0.77 | time 4.29 sec\n","epoch 239 | train_loss 0.5817 | test_acc 0.73 | time 4.46 sec\n","epoch 240 | train_loss 0.5775 | test_acc 0.73 | time 4.32 sec\n","epoch 241 | train_loss 0.6175 | test_acc 0.70 | time 4.23 sec\n","epoch 242 | train_loss 0.5340 | test_acc 0.57 | time 5.45 sec\n","epoch 243 | train_loss 0.5184 | test_acc 0.70 | time 4.36 sec\n","epoch 244 | train_loss 0.5616 | test_acc 0.70 | time 4.39 sec\n","epoch 245 | train_loss 0.4849 | test_acc 0.73 | time 4.37 sec\n","epoch 246 | train_loss 0.5197 | test_acc 0.70 | time 7.30 sec\n","epoch 247 | train_loss 0.5569 | test_acc 0.70 | time 4.30 sec\n","epoch 248 | train_loss 0.5661 | test_acc 0.67 | time 4.18 sec\n","epoch 249 | train_loss 0.4997 | test_acc 0.63 | time 4.41 sec\n","epoch 250 | train_loss 0.5008 | test_acc 0.67 | time 4.34 sec\n","epoch 251 | train_loss 0.4883 | test_acc 0.70 | time 4.36 sec\n","epoch 252 | train_loss 0.5407 | test_acc 0.63 | time 4.31 sec\n","epoch 253 | train_loss 0.5531 | test_acc 0.53 | time 4.30 sec\n","epoch 254 | train_loss 0.6021 | test_acc 0.53 | time 4.75 sec\n","epoch 255 | train_loss 0.6306 | test_acc 0.53 | time 4.30 sec\n","epoch 256 | train_loss 0.5418 | test_acc 0.70 | time 4.45 sec\n","epoch 257 | train_loss 0.5480 | test_acc 0.70 | time 4.37 sec\n","epoch 258 | train_loss 0.4970 | test_acc 0.63 | time 4.36 sec\n","epoch 259 | train_loss 0.5457 | test_acc 0.63 | time 4.38 sec\n","epoch 260 | train_loss 0.5663 | test_acc 0.60 | time 5.65 sec\n","epoch 261 | train_loss 0.6052 | test_acc 0.60 | time 7.83 sec\n","epoch 262 | train_loss 0.5826 | test_acc 0.57 | time 4.43 sec\n","epoch 263 | train_loss 0.6210 | test_acc 0.63 | time 4.46 sec\n","epoch 264 | train_loss 0.5881 | test_acc 0.60 | time 4.36 sec\n","epoch 265 | train_loss 0.6283 | test_acc 0.60 | time 4.21 sec\n","epoch 266 | train_loss 0.5667 | test_acc 0.63 | time 4.91 sec\n","epoch 267 | train_loss 0.5403 | test_acc 0.73 | time 4.34 sec\n","epoch 268 | train_loss 0.5313 | test_acc 0.70 | time 4.32 sec\n","epoch 269 | train_loss 0.5962 | test_acc 0.73 | time 4.56 sec\n","epoch 270 | train_loss 0.4964 | test_acc 0.70 | time 4.32 sec\n","epoch 271 | train_loss 0.5055 | test_acc 0.63 | time 4.94 sec\n","epoch 272 | train_loss 0.4521 | test_acc 0.73 | time 4.90 sec\n","epoch 273 | train_loss 0.5166 | test_acc 0.77 | time 4.74 sec\n","epoch 274 | train_loss 0.4804 | test_acc 0.73 | time 5.18 sec\n","epoch 275 | train_loss 0.4860 | test_acc 0.73 | time 4.29 sec\n","epoch 276 | train_loss 0.4865 | test_acc 0.77 | time 4.27 sec\n","epoch 277 | train_loss 0.4896 | test_acc 0.80 | time 5.75 sec\n","epoch 278 | train_loss 0.5820 | test_acc 0.77 | time 4.42 sec\n","epoch 279 | train_loss 0.5242 | test_acc 0.73 | time 4.33 sec\n","epoch 280 | train_loss 0.5471 | test_acc 0.70 | time 4.22 sec\n","epoch 281 | train_loss 0.5015 | test_acc 0.70 | time 4.36 sec\n","epoch 282 | train_loss 0.4754 | test_acc 0.67 | time 4.35 sec\n","epoch 283 | train_loss 0.5031 | test_acc 0.67 | time 4.38 sec\n","epoch 284 | train_loss 0.4690 | test_acc 0.70 | time 4.39 sec\n","epoch 285 | train_loss 0.4922 | test_acc 0.67 | time 5.71 sec\n","epoch 286 | train_loss 0.4610 | test_acc 0.67 | time 4.54 sec\n","epoch 287 | train_loss 0.4616 | test_acc 0.77 | time 6.85 sec\n","epoch 288 | train_loss 0.5122 | test_acc 0.70 | time 6.73 sec\n","epoch 289 | train_loss 0.4881 | test_acc 0.67 | time 4.44 sec\n","epoch 290 | train_loss 0.4644 | test_acc 0.67 | time 4.37 sec\n","epoch 291 | train_loss 0.4337 | test_acc 0.77 | time 4.32 sec\n","epoch 292 | train_loss 0.5041 | test_acc 0.70 | time 4.22 sec\n","epoch 293 | train_loss 0.4404 | test_acc 0.73 | time 4.41 sec\n","epoch 294 | train_loss 0.5786 | test_acc 0.77 | time 4.52 sec\n","epoch 295 | train_loss 0.4418 | test_acc 0.60 | time 4.48 sec\n","epoch 296 | train_loss 0.4508 | test_acc 0.60 | time 4.24 sec\n","epoch 297 | train_loss 0.5228 | test_acc 0.67 | time 10.17 sec\n","epoch 298 | train_loss 0.5334 | test_acc 0.60 | time 4.45 sec\n","epoch 299 | train_loss 0.5618 | test_acc 0.43 | time 4.33 sec\n","epoch 300 | train_loss 0.5921 | test_acc 0.57 | time 4.41 sec\n","epoch 301 | train_loss 0.6373 | test_acc 0.63 | time 4.31 sec\n","epoch 302 | train_loss 0.6685 | test_acc 0.63 | time 4.82 sec\n","epoch 303 | train_loss 0.6081 | test_acc 0.67 | time 4.43 sec\n","epoch 304 | train_loss 0.7128 | test_acc 0.60 | time 4.26 sec\n","epoch 305 | train_loss 0.6507 | test_acc 0.73 | time 4.58 sec\n","epoch 306 | train_loss 0.7313 | test_acc 0.73 | time 4.48 sec\n","epoch 307 | train_loss 0.6424 | test_acc 0.73 | time 4.29 sec\n","epoch 308 | train_loss 0.6008 | test_acc 0.73 | time 4.42 sec\n","epoch 309 | train_loss 0.6759 | test_acc 0.60 | time 4.58 sec\n","epoch 310 | train_loss 0.6013 | test_acc 0.63 | time 4.52 sec\n","epoch 311 | train_loss 0.6208 | test_acc 0.70 | time 4.35 sec\n","epoch 312 | train_loss 0.6802 | test_acc 0.73 | time 4.43 sec\n","epoch 313 | train_loss 0.6373 | test_acc 0.67 | time 4.78 sec\n","epoch 314 | train_loss 0.6603 | test_acc 0.63 | time 5.01 sec\n","epoch 315 | train_loss 0.6276 | test_acc 0.70 | time 4.41 sec\n","epoch 316 | train_loss 0.6135 | test_acc 0.77 | time 4.43 sec\n","epoch 317 | train_loss 0.5832 | test_acc 0.77 | time 4.44 sec\n","epoch 318 | train_loss 0.6376 | test_acc 0.80 | time 4.38 sec\n","epoch 319 | train_loss 0.5567 | test_acc 0.70 | time 4.49 sec\n","epoch 320 | train_loss 0.5355 | test_acc 0.73 | time 4.48 sec\n","epoch 321 | train_loss 0.5840 | test_acc 0.70 | time 4.39 sec\n","epoch 322 | train_loss 0.6034 | test_acc 0.77 | time 4.66 sec\n","epoch 323 | train_loss 0.5877 | test_acc 0.73 | time 4.79 sec\n","epoch 324 | train_loss 0.5311 | test_acc 0.73 | time 4.42 sec\n","epoch 325 | train_loss 0.5754 | test_acc 0.73 | time 4.39 sec\n","epoch 326 | train_loss 0.5830 | test_acc 0.70 | time 4.35 sec\n","epoch 327 | train_loss 0.5492 | test_acc 0.73 | time 4.60 sec\n","epoch 328 | train_loss 0.4874 | test_acc 0.73 | time 4.59 sec\n","epoch 329 | train_loss 0.5310 | test_acc 0.67 | time 4.54 sec\n","epoch 330 | train_loss 0.5639 | test_acc 0.70 | time 8.00 sec\n","epoch 331 | train_loss 0.5528 | test_acc 0.77 | time 4.28 sec\n","epoch 332 | train_loss 0.5834 | test_acc 0.70 | time 4.55 sec\n","epoch 333 | train_loss 0.5242 | test_acc 0.73 | time 4.55 sec\n","epoch 334 | train_loss 0.5070 | test_acc 0.73 | time 4.39 sec\n","epoch 335 | train_loss 0.5188 | test_acc 0.70 | time 10.52 sec\n","epoch 336 | train_loss 0.5168 | test_acc 0.67 | time 4.34 sec\n","epoch 337 | train_loss 0.5474 | test_acc 0.70 | time 4.48 sec\n","epoch 338 | train_loss 0.5290 | test_acc 0.70 | time 4.79 sec\n","epoch 339 | train_loss 0.5535 | test_acc 0.63 | time 4.41 sec\n","epoch 340 | train_loss 0.5663 | test_acc 0.70 | time 4.53 sec\n","epoch 341 | train_loss 0.5634 | test_acc 0.80 | time 4.94 sec\n","epoch 342 | train_loss 0.5946 | test_acc 0.70 | time 4.45 sec\n","epoch 343 | train_loss 0.4825 | test_acc 0.77 | time 4.32 sec\n","epoch 344 | train_loss 0.4811 | test_acc 0.73 | time 4.42 sec\n","epoch 345 | train_loss 0.5114 | test_acc 0.63 | time 4.47 sec\n","epoch 346 | train_loss 0.4924 | test_acc 0.53 | time 4.49 sec\n","epoch 347 | train_loss 0.5502 | test_acc 0.50 | time 4.41 sec\n","epoch 348 | train_loss 0.5918 | test_acc 0.73 | time 6.36 sec\n","epoch 349 | train_loss 0.4648 | test_acc 0.73 | time 4.41 sec\n","epoch 350 | train_loss 0.5041 | test_acc 0.73 | time 4.56 sec\n","epoch 351 | train_loss 0.5495 | test_acc 0.70 | time 4.46 sec\n","epoch 352 | train_loss 0.5055 | test_acc 0.70 | time 6.96 sec\n","epoch 353 | train_loss 0.5045 | test_acc 0.73 | time 4.39 sec\n","epoch 354 | train_loss 0.5164 | test_acc 0.73 | time 4.42 sec\n","epoch 355 | train_loss 0.4686 | test_acc 0.77 | time 4.40 sec\n","epoch 356 | train_loss 0.5515 | test_acc 0.60 | time 4.56 sec\n","epoch 357 | train_loss 0.5010 | test_acc 0.70 | time 4.35 sec\n","epoch 358 | train_loss 0.5395 | test_acc 0.73 | time 4.37 sec\n","epoch 359 | train_loss 0.5174 | test_acc 0.73 | time 4.33 sec\n","epoch 360 | train_loss 0.5130 | test_acc 0.80 | time 4.44 sec\n","epoch 361 | train_loss 0.4289 | test_acc 0.80 | time 8.51 sec\n","epoch 362 | train_loss 0.4673 | test_acc 0.73 | time 4.36 sec\n","epoch 363 | train_loss 0.5351 | test_acc 0.70 | time 4.32 sec\n","epoch 364 | train_loss 0.4783 | test_acc 0.67 | time 4.39 sec\n","epoch 365 | train_loss 0.5044 | test_acc 0.67 | time 4.45 sec\n","epoch 366 | train_loss 0.5006 | test_acc 0.73 | time 4.40 sec\n","epoch 367 | train_loss 0.5522 | test_acc 0.73 | time 4.50 sec\n","epoch 368 | train_loss 0.5314 | test_acc 0.77 | time 4.46 sec\n","epoch 369 | train_loss 0.5721 | test_acc 0.73 | time 4.43 sec\n","epoch 370 | train_loss 0.4389 | test_acc 0.70 | time 4.28 sec\n","epoch 371 | train_loss 0.4387 | test_acc 0.70 | time 4.44 sec\n","epoch 372 | train_loss 0.4914 | test_acc 0.70 | time 4.29 sec\n","epoch 373 | train_loss 0.5033 | test_acc 0.70 | time 4.33 sec\n","epoch 374 | train_loss 0.4402 | test_acc 0.60 | time 6.98 sec\n","epoch 375 | train_loss 0.5284 | test_acc 0.57 | time 4.40 sec\n","epoch 376 | train_loss 0.5092 | test_acc 0.60 | time 4.46 sec\n","epoch 377 | train_loss 0.4899 | test_acc 0.67 | time 6.24 sec\n","epoch 378 | train_loss 0.4827 | test_acc 0.67 | time 4.49 sec\n","epoch 379 | train_loss 0.4979 | test_acc 0.70 | time 4.43 sec\n","epoch 380 | train_loss 0.5352 | test_acc 0.70 | time 4.23 sec\n","epoch 381 | train_loss 0.5506 | test_acc 0.73 | time 4.53 sec\n","epoch 382 | train_loss 0.5363 | test_acc 0.57 | time 4.26 sec\n","epoch 383 | train_loss 0.5394 | test_acc 0.63 | time 4.46 sec\n","epoch 384 | train_loss 0.5799 | test_acc 0.57 | time 4.25 sec\n","epoch 385 | train_loss 0.5747 | test_acc 0.53 | time 4.60 sec\n","epoch 386 | train_loss 0.5541 | test_acc 0.57 | time 4.64 sec\n","epoch 387 | train_loss 0.5032 | test_acc 0.57 | time 10.06 sec\n","epoch 388 | train_loss 0.5201 | test_acc 0.57 | time 4.38 sec\n","epoch 389 | train_loss 0.4892 | test_acc 0.53 | time 4.20 sec\n","epoch 390 | train_loss 0.5630 | test_acc 0.67 | time 4.48 sec\n","epoch 391 | train_loss 0.5097 | test_acc 0.73 | time 4.29 sec\n","epoch 392 | train_loss 0.4973 | test_acc 0.47 | time 4.36 sec\n","epoch 393 | train_loss 0.5258 | test_acc 0.73 | time 4.53 sec\n","epoch 394 | train_loss 0.5120 | test_acc 0.70 | time 4.32 sec\n","epoch 395 | train_loss 0.4406 | test_acc 0.73 | time 4.44 sec\n","epoch 396 | train_loss 0.5016 | test_acc 0.73 | time 4.39 sec\n","epoch 397 | train_loss 0.5729 | test_acc 0.60 | time 5.76 sec\n","epoch 398 | train_loss 0.5190 | test_acc 0.47 | time 4.41 sec\n","epoch 399 | train_loss 0.5839 | test_acc 0.60 | time 4.45 sec\n","epoch 400 | train_loss 0.6337 | test_acc 0.63 | time 4.55 sec\n","epoch 401 | train_loss 0.5482 | test_acc 0.67 | time 7.42 sec\n","epoch 402 | train_loss 0.5102 | test_acc 0.57 | time 4.43 sec\n","epoch 403 | train_loss 0.5777 | test_acc 0.60 | time 4.41 sec\n","epoch 404 | train_loss 0.5994 | test_acc 0.63 | time 4.54 sec\n","epoch 405 | train_loss 0.5801 | test_acc 0.53 | time 4.21 sec\n","epoch 406 | train_loss 0.5972 | test_acc 0.57 | time 4.49 sec\n","epoch 407 | train_loss 0.5125 | test_acc 0.67 | time 4.54 sec\n","epoch 408 | train_loss 0.4992 | test_acc 0.63 | time 4.30 sec\n","epoch 409 | train_loss 0.5409 | test_acc 0.60 | time 4.37 sec\n","epoch 410 | train_loss 0.5476 | test_acc 0.57 | time 4.49 sec\n","epoch 411 | train_loss 0.5118 | test_acc 0.67 | time 4.73 sec\n","epoch 412 | train_loss 0.4781 | test_acc 0.67 | time 4.38 sec\n","epoch 413 | train_loss 0.5484 | test_acc 0.70 | time 4.44 sec\n","epoch 414 | train_loss 0.5233 | test_acc 0.70 | time 4.47 sec\n","epoch 415 | train_loss 0.5589 | test_acc 0.63 | time 4.49 sec\n","epoch 416 | train_loss 0.5097 | test_acc 0.67 | time 6.14 sec\n","epoch 417 | train_loss 0.5390 | test_acc 0.63 | time 4.20 sec\n","epoch 418 | train_loss 0.5416 | test_acc 0.63 | time 4.45 sec\n","epoch 419 | train_loss 0.4911 | test_acc 0.63 | time 4.40 sec\n","epoch 420 | train_loss 0.4721 | test_acc 0.67 | time 4.72 sec\n","epoch 421 | train_loss 0.5064 | test_acc 0.70 | time 4.35 sec\n","epoch 422 | train_loss 0.4809 | test_acc 0.70 | time 4.40 sec\n","epoch 423 | train_loss 0.5295 | test_acc 0.73 | time 4.31 sec\n","epoch 424 | train_loss 0.5216 | test_acc 0.77 | time 4.52 sec\n","epoch 425 | train_loss 0.4937 | test_acc 0.70 | time 4.46 sec\n","epoch 426 | train_loss 0.4898 | test_acc 0.67 | time 4.42 sec\n","epoch 427 | train_loss 0.5483 | test_acc 0.70 | time 4.36 sec\n","epoch 428 | train_loss 0.5892 | test_acc 0.60 | time 4.44 sec\n","epoch 429 | train_loss 0.6238 | test_acc 0.67 | time 4.34 sec\n","epoch 430 | train_loss 0.6398 | test_acc 0.80 | time 5.80 sec\n","epoch 431 | train_loss 0.7316 | test_acc 0.53 | time 5.86 sec\n","epoch 432 | train_loss 0.8170 | test_acc 0.63 | time 4.45 sec\n","epoch 433 | train_loss 0.8749 | test_acc 0.67 | time 4.38 sec\n","epoch 434 | train_loss 0.8594 | test_acc 0.60 | time 4.45 sec\n","epoch 435 | train_loss 0.7125 | test_acc 0.60 | time 4.37 sec\n","epoch 436 | train_loss 0.8282 | test_acc 0.60 | time 4.52 sec\n","epoch 437 | train_loss 0.7015 | test_acc 0.63 | time 4.42 sec\n","epoch 438 | train_loss 0.5906 | test_acc 0.77 | time 4.44 sec\n","epoch 439 | train_loss 0.5830 | test_acc 0.77 | time 4.43 sec\n","epoch 440 | train_loss 0.5686 | test_acc 0.67 | time 4.49 sec\n","epoch 441 | train_loss 0.5969 | test_acc 0.70 | time 4.44 sec\n","epoch 442 | train_loss 0.5879 | test_acc 0.67 | time 4.58 sec\n","epoch 443 | train_loss 0.5840 | test_acc 0.67 | time 4.44 sec\n","epoch 444 | train_loss 0.6065 | test_acc 0.63 | time 8.12 sec\n","epoch 445 | train_loss 0.4883 | test_acc 0.67 | time 4.52 sec\n","epoch 446 | train_loss 0.4822 | test_acc 0.63 | time 6.84 sec\n","epoch 447 | train_loss 0.5534 | test_acc 0.70 | time 4.49 sec\n","epoch 448 | train_loss 0.5710 | test_acc 0.60 | time 4.52 sec\n","epoch 449 | train_loss 0.5518 | test_acc 0.70 | time 4.50 sec\n","epoch 450 | train_loss 0.6132 | test_acc 0.70 | time 8.91 sec\n","epoch 451 | train_loss 0.5730 | test_acc 0.70 | time 4.43 sec\n","epoch 452 | train_loss 0.5393 | test_acc 0.60 | time 4.51 sec\n","epoch 453 | train_loss 0.5261 | test_acc 0.57 | time 4.43 sec\n","epoch 454 | train_loss 0.5798 | test_acc 0.63 | time 4.34 sec\n","epoch 455 | train_loss 0.5263 | test_acc 0.57 | time 4.49 sec\n","epoch 456 | train_loss 0.6147 | test_acc 0.63 | time 4.43 sec\n","epoch 457 | train_loss 0.6207 | test_acc 0.73 | time 4.43 sec\n","epoch 458 | train_loss 0.5587 | test_acc 0.73 | time 4.41 sec\n","epoch 459 | train_loss 0.6064 | test_acc 0.73 | time 7.97 sec\n","epoch 460 | train_loss 0.6261 | test_acc 0.73 | time 4.53 sec\n","epoch 461 | train_loss 0.6120 | test_acc 0.73 | time 4.45 sec\n","epoch 462 | train_loss 0.5150 | test_acc 0.73 | time 4.49 sec\n","epoch 463 | train_loss 0.5885 | test_acc 0.73 | time 4.78 sec\n","epoch 464 | train_loss 0.5052 | test_acc 0.70 | time 4.32 sec\n","epoch 465 | train_loss 0.6247 | test_acc 0.73 | time 4.35 sec\n","epoch 466 | train_loss 0.5503 | test_acc 0.67 | time 4.35 sec\n","epoch 467 | train_loss 0.5370 | test_acc 0.63 | time 7.99 sec\n","epoch 468 | train_loss 0.5679 | test_acc 0.67 | time 4.40 sec\n","epoch 469 | train_loss 0.5712 | test_acc 0.67 | time 4.44 sec\n","epoch 470 | train_loss 0.5915 | test_acc 0.67 | time 4.91 sec\n","epoch 471 | train_loss 0.6710 | test_acc 0.60 | time 4.40 sec\n","epoch 472 | train_loss 0.7118 | test_acc 0.60 | time 4.40 sec\n","epoch 473 | train_loss 0.6869 | test_acc 0.67 | time 5.00 sec\n","epoch 474 | train_loss 0.6715 | test_acc 0.70 | time 6.42 sec\n","epoch 475 | train_loss 0.6264 | test_acc 0.70 | time 4.44 sec\n","epoch 476 | train_loss 0.6756 | test_acc 0.70 | time 4.34 sec\n","epoch 477 | train_loss 0.6051 | test_acc 0.73 | time 4.49 sec\n","epoch 478 | train_loss 0.5575 | test_acc 0.73 | time 4.37 sec\n","epoch 479 | train_loss 0.5844 | test_acc 0.70 | time 4.54 sec\n","epoch 480 | train_loss 0.5988 | test_acc 0.70 | time 4.39 sec\n","epoch 481 | train_loss 0.5363 | test_acc 0.73 | time 4.43 sec\n","epoch 482 | train_loss 0.5519 | test_acc 0.77 | time 4.41 sec\n","epoch 483 | train_loss 0.5634 | test_acc 0.70 | time 4.35 sec\n","epoch 484 | train_loss 0.5307 | test_acc 0.77 | time 4.48 sec\n","epoch 485 | train_loss 0.4939 | test_acc 0.77 | time 4.45 sec\n","epoch 486 | train_loss 0.5229 | test_acc 0.77 | time 4.36 sec\n","epoch 487 | train_loss 0.5168 | test_acc 0.70 | time 4.38 sec\n","epoch 488 | train_loss 0.5632 | test_acc 0.67 | time 4.32 sec\n","epoch 489 | train_loss 0.5358 | test_acc 0.67 | time 4.37 sec\n","epoch 490 | train_loss 0.4948 | test_acc 0.77 | time 8.22 sec\n","epoch 491 | train_loss 0.5136 | test_acc 0.77 | time 4.44 sec\n","epoch 492 | train_loss 0.5245 | test_acc 0.77 | time 4.37 sec\n","epoch 493 | train_loss 0.4464 | test_acc 0.77 | time 4.95 sec\n","epoch 494 | train_loss 0.5394 | test_acc 0.77 | time 9.61 sec\n","epoch 495 | train_loss 0.5523 | test_acc 0.70 | time 4.54 sec\n","epoch 496 | train_loss 0.4642 | test_acc 0.67 | time 4.20 sec\n","epoch 497 | train_loss 0.4941 | test_acc 0.73 | time 4.38 sec\n","epoch 498 | train_loss 0.5878 | test_acc 0.63 | time 4.37 sec\n","epoch 499 | train_loss 0.5798 | test_acc 0.77 | time 9.73 sec\n","epoch 500 | train_loss 0.6022 | test_acc 0.73 | time 4.44 sec\n","epoch 501 | train_loss 0.5420 | test_acc 0.77 | time 4.52 sec\n","epoch 502 | train_loss 0.5503 | test_acc 0.73 | time 4.40 sec\n","epoch 503 | train_loss 0.4648 | test_acc 0.77 | time 4.53 sec\n","epoch 504 | train_loss 0.5024 | test_acc 0.77 | time 5.93 sec\n","epoch 505 | train_loss 0.5658 | test_acc 0.77 | time 4.62 sec\n","epoch 506 | train_loss 0.5350 | test_acc 0.67 | time 4.90 sec\n","epoch 507 | train_loss 0.4568 | test_acc 0.70 | time 5.50 sec\n","epoch 508 | train_loss 0.5132 | test_acc 0.70 | time 4.48 sec\n","epoch 509 | train_loss 0.4612 | test_acc 0.70 | time 7.12 sec\n","epoch 510 | train_loss 0.6029 | test_acc 0.63 | time 5.75 sec\n","epoch 511 | train_loss 0.4656 | test_acc 0.67 | time 4.37 sec\n","epoch 512 | train_loss 0.5048 | test_acc 0.67 | time 7.36 sec\n","epoch 513 | train_loss 0.5297 | test_acc 0.67 | time 6.04 sec\n","epoch 514 | train_loss 0.5743 | test_acc 0.70 | time 4.47 sec\n","epoch 515 | train_loss 0.5926 | test_acc 0.70 | time 4.97 sec\n","epoch 516 | train_loss 0.5627 | test_acc 0.57 | time 7.47 sec\n","epoch 517 | train_loss 0.5815 | test_acc 0.50 | time 5.76 sec\n","epoch 518 | train_loss 0.6546 | test_acc 0.50 | time 4.45 sec\n","epoch 519 | train_loss 0.5042 | test_acc 0.47 | time 4.76 sec\n","epoch 520 | train_loss 0.4920 | test_acc 0.67 | time 4.51 sec\n","epoch 521 | train_loss 0.5237 | test_acc 0.67 | time 6.69 sec\n","epoch 522 | train_loss 0.5052 | test_acc 0.43 | time 6.30 sec\n","epoch 523 | train_loss 0.6076 | test_acc 0.50 | time 4.45 sec\n","epoch 524 | train_loss 0.5380 | test_acc 0.50 | time 5.07 sec\n","epoch 525 | train_loss 0.5873 | test_acc 0.70 | time 7.37 sec\n","epoch 526 | train_loss 0.6136 | test_acc 0.77 | time 5.14 sec\n","epoch 527 | train_loss 0.5706 | test_acc 0.73 | time 4.46 sec\n","epoch 528 | train_loss 0.5468 | test_acc 0.40 | time 4.36 sec\n","epoch 529 | train_loss 0.4970 | test_acc 0.43 | time 4.49 sec\n","epoch 530 | train_loss 0.5587 | test_acc 0.53 | time 4.40 sec\n","epoch 531 | train_loss 0.6050 | test_acc 0.57 | time 4.54 sec\n","epoch 532 | train_loss 0.5805 | test_acc 0.70 | time 4.56 sec\n","epoch 533 | train_loss 0.5154 | test_acc 0.70 | time 4.42 sec\n","epoch 534 | train_loss 0.6116 | test_acc 0.70 | time 9.49 sec\n","epoch 535 | train_loss 0.5342 | test_acc 0.70 | time 4.38 sec\n","epoch 536 | train_loss 0.5036 | test_acc 0.70 | time 4.36 sec\n","epoch 537 | train_loss 0.4876 | test_acc 0.73 | time 4.48 sec\n","epoch 538 | train_loss 0.5001 | test_acc 0.73 | time 4.47 sec\n","epoch 539 | train_loss 0.5029 | test_acc 0.67 | time 7.29 sec\n","epoch 540 | train_loss 0.4924 | test_acc 0.73 | time 4.43 sec\n","epoch 541 | train_loss 0.4628 | test_acc 0.73 | time 4.49 sec\n","epoch 542 | train_loss 0.5024 | test_acc 0.77 | time 4.45 sec\n","epoch 543 | train_loss 0.4827 | test_acc 0.70 | time 4.58 sec\n","epoch 544 | train_loss 0.4943 | test_acc 0.70 | time 4.56 sec\n","epoch 545 | train_loss 0.4621 | test_acc 0.70 | time 4.49 sec\n","epoch 546 | train_loss 0.4545 | test_acc 0.63 | time 4.39 sec\n","epoch 547 | train_loss 0.5009 | test_acc 0.57 | time 4.91 sec\n","epoch 548 | train_loss 0.5308 | test_acc 0.70 | time 4.35 sec\n","epoch 549 | train_loss 0.5358 | test_acc 0.57 | time 4.37 sec\n","epoch 550 | train_loss 0.6156 | test_acc 0.53 | time 4.51 sec\n","epoch 551 | train_loss 0.5777 | test_acc 0.60 | time 4.54 sec\n","epoch 552 | train_loss 0.5763 | test_acc 0.67 | time 4.39 sec\n","epoch 553 | train_loss 0.5406 | test_acc 0.70 | time 4.31 sec\n","epoch 554 | train_loss 0.5358 | test_acc 0.70 | time 4.50 sec\n","epoch 555 | train_loss 0.5621 | test_acc 0.70 | time 5.03 sec\n","epoch 556 | train_loss 0.5286 | test_acc 0.73 | time 4.46 sec\n","epoch 557 | train_loss 0.4878 | test_acc 0.73 | time 4.48 sec\n","epoch 558 | train_loss 0.5791 | test_acc 0.70 | time 4.50 sec\n","epoch 559 | train_loss 0.5580 | test_acc 0.67 | time 4.45 sec\n","epoch 560 | train_loss 0.5511 | test_acc 0.70 | time 4.39 sec\n","epoch 561 | train_loss 0.5378 | test_acc 0.70 | time 4.23 sec\n","epoch 562 | train_loss 0.5091 | test_acc 0.80 | time 4.46 sec\n","epoch 563 | train_loss 0.5225 | test_acc 0.73 | time 6.07 sec\n","epoch 564 | train_loss 0.4961 | test_acc 0.70 | time 4.55 sec\n","epoch 565 | train_loss 0.4569 | test_acc 0.70 | time 4.34 sec\n","epoch 566 | train_loss 0.4777 | test_acc 0.73 | time 4.35 sec\n","epoch 567 | train_loss 0.4947 | test_acc 0.73 | time 4.35 sec\n","epoch 568 | train_loss 0.5888 | test_acc 0.70 | time 4.38 sec\n","epoch 569 | train_loss 0.5108 | test_acc 0.67 | time 7.91 sec\n","epoch 570 | train_loss 0.4721 | test_acc 0.70 | time 4.43 sec\n","epoch 571 | train_loss 0.4721 | test_acc 0.70 | time 4.35 sec\n","epoch 572 | train_loss 0.5096 | test_acc 0.70 | time 4.97 sec\n","epoch 573 | train_loss 0.4579 | test_acc 0.73 | time 4.93 sec\n","epoch 574 | train_loss 0.4718 | test_acc 0.77 | time 4.60 sec\n","epoch 575 | train_loss 0.5062 | test_acc 0.80 | time 4.51 sec\n","epoch 576 | train_loss 0.4901 | test_acc 0.80 | time 5.12 sec\n","epoch 577 | train_loss 0.5101 | test_acc 0.83 | time 4.45 sec\n","epoch 578 | train_loss 0.4954 | test_acc 0.70 | time 4.41 sec\n","epoch 579 | train_loss 0.5114 | test_acc 0.70 | time 4.41 sec\n","epoch 580 | train_loss 0.5084 | test_acc 0.73 | time 4.36 sec\n","epoch 581 | train_loss 0.4696 | test_acc 0.73 | time 4.57 sec\n","epoch 582 | train_loss 0.4973 | test_acc 0.73 | time 4.52 sec\n","epoch 583 | train_loss 0.4719 | test_acc 0.77 | time 4.62 sec\n","epoch 584 | train_loss 0.5304 | test_acc 0.80 | time 10.49 sec\n","epoch 585 | train_loss 0.5273 | test_acc 0.83 | time 4.39 sec\n","epoch 586 | train_loss 0.4590 | test_acc 0.80 | time 4.45 sec\n","epoch 587 | train_loss 0.5424 | test_acc 0.90 | time 4.48 sec\n","model saving...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type ResNet_50. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Bottleneck. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Flatten. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e8497a750a19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_list_ep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_list_ep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m \u001b[0mtrain_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresnet_50_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresnet_50_acc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-e8497a750a19>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[0;34m(net, train_loader, test_loader, loss_list, lr, num_epochs, acc_list)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mlabels_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-e8497a750a19>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mx_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mx_conv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_conv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mx_conv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_conv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 346\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"]}]},{"cell_type":"code","metadata":{"id":"gEEItqvXmUxC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a5326ef0-38fc-43ec-c670-f01f4fea9c3c","executionInfo":{"status":"ok","timestamp":1589685598070,"user_tz":-480,"elapsed":1203,"user":{"displayName":"zqstrive","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggr24K_aoT3UYoqlbf6c5wwDOD052P8YoqRDHGp-A=s64","userId":"15827624022674906427"}}},"source":["\n","from PIL import Image\n","\n","\n","img = Image.open(path)\n","print(img.size)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["(640, 480)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f99NOGKzpmRN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"295c7a17-c798-412c-e2eb-31933dfde702","executionInfo":{"status":"ok","timestamp":1589685850129,"user_tz":-480,"elapsed":1253,"user":{"displayName":"zqstrive","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggr24K_aoT3UYoqlbf6c5wwDOD052P8YoqRDHGp-A=s64","userId":"15827624022674906427"}}},"source":["import os\n","import cv2\n","import numpy as np\n","path = os.path.join('/content/drive/My Drive/QML/cv/image_classification/train set','1.jpg')\n","img = cv2.imread(path)\n","sp = img.shape\n","print(sp)\n","sz1 = sp[0]#height(rows) of image\n","sz2 = sp[1]#width(colums) of image\n","sz3 = sp[2]#the pixels value is made up of three primary colors\n","print('width: %d \\nheight: %d \\nnumber: %d'%(sz1,sz2,sz3))\n","    \n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["(480, 640, 3)\n","width: 480 \n","height: 640 \n","number: 3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zwQmZ0CScskj","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"j6TMWq_Ccsvw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":295},"outputId":"5adb5d7a-0844-4149-c45a-84f9387bcbf2","executionInfo":{"status":"ok","timestamp":1589700482439,"user_tz":-480,"elapsed":4041,"user":{"displayName":"zqstrive","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggr24K_aoT3UYoqlbf6c5wwDOD052P8YoqRDHGp-A=s64","userId":"15827624022674906427"}}},"source":["# lib that need\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","import os\n","import scipy.io as sio\n","from torch.autograd import Variable\n","from tqdm import tqdm\n","\n","from PIL import Image\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","# says it can accelerate the model\n","torch.backends.cudnn.benchmark = True\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')\n","\n","# data preprocess  -> 224X224 like resnet paper use\n","transform = transforms.Compose(\n","    [transforms.Resize(1024),\n","     transforms.CenterCrop(224),\n","     transforms.ToTensor(),\n","     transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])]\n",")\n","\n","class MyDataset(Dataset):\n","    def __init__(self,root,transform=None):\n","        super(MyDataset,self).__init__()\n","        self.root = root\n","        self.img_list = os.listdir(root)\n","        self.transform = transform\n","\n","    def __getitem__(self,index):\n","        img_name = os.path.join(self.root,self.img_list[index])\n","        if self.img_list[index][0] == 'h':\n","            label = 1\n","        else:\n","            label = 0\n","        img = Image.open(img_name)\n","        # img_tensor = transforms.ToTensor(img)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        return img,label\n","\n","    def __len__(self):\n","        return len(self.img_list)\n","\n","# dataset and dataloader\n","train_set = MyDataset(root='/content/drive/My Drive/QML/cv/image_classification/train set',transform=transform)\n","test_set = MyDataset(root='/content/drive/My Drive/QML/cv/image_classification/test set',transform=transform)\n","\n","batch_size = 10\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True,num_workers=4)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False,num_workers=4)\n","\n","# for (inputs, labels) in train_loader:\n","#     print(inputs.shape)\n","#     print(labels)\n","\n","\n","\n","# flatten the output to  full-connection\n","def flatten(x):\n","    N = x.shape[0]  # x -> N,C,H,W\n","    return x.view(N,-1)\n","class Flatten(nn.Module):\n","    def forward(self,x):\n","        return flatten(x)\n","\n","# classical convolution network\n","class LeNet(nn.Module):\n","    def __init__(self):\n","        super(LeNet, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 6, 5),\n","            nn.BatchNorm2d(6),\n","            nn.ReLU(inplace=True),  # reduce memory\n","            nn.MaxPool2d(2, 2)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(6, 16, 5),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, 2)\n","        )\n","        self.fc = nn.Sequential(\n","            Flatten(),\n","            nn.Linear(16 * 5 * 5, 120),\n","            nn.BatchNorm1d(120),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(120, 84),\n","            nn.BatchNorm1d(84),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(84, 10)\n","        )\n","\n","    def forward(self, x):\n","        x_conv1 = self.conv1(x)\n","        x_conv2 = self.conv2(x_conv1)\n","        # x_conv2 = x_conv2.view(-1, 16 * 5 * 5)\n","        x_fc = self.fc(x_conv2)\n","        return x_fc\n","\n","# residual block\n","class Bottleneck(nn.Module):\n","    def __init__(self, in_channels, stride=1, first_layer=False,down_sampling=False,next_layer=False,groups=1,depth=1):\n","        \"\"\"\n","\n","        :param in_channels: in_channels\n","        :param stride:stride of 3x3 conv\n","        :param first_layer:conv with no down_sampling\n","        :param down_sampling:down_sampling\n","        :param next_layer:different block of conv\n","        :param groups:3x3 conv groups\n","        :param depth:residual block depth\n","        \"\"\"\n","        super(Bottleneck, self).__init__()\n","        change_channels = 4*in_channels\n","        self.downsampling = down_sampling\n","        self.first_layer = first_layer\n","\n","        base_channels = 64      # base-channels\n","        if groups == 1:         # resnet\n","            width_channels = in_channels\n","            down_sampling_channels = 2*in_channels\n","        elif groups == 32:      # resnext\n","            width_channels = np.int(in_channels/base_channels)*depth*groups\n","            down_sampling_channels = width_channels\n","        if first_layer:\n","            self.conv1 = nn.Sequential(\n","                nn.Conv2d(in_channels,width_channels , kernel_size=1),\n","                nn.BatchNorm2d(width_channels),\n","                nn.ReLU(inplace=True)   # in_place to reduce memory\n","            )\n","        elif next_layer:\n","            self.conv1 = nn.Sequential(\n","                nn.Conv2d(down_sampling_channels, width_channels, kernel_size=1),\n","                nn.BatchNorm2d(width_channels),\n","                nn.ReLU(inplace=True)\n","            )\n","        else:\n","            self.conv1 = nn.Sequential(\n","                nn.Conv2d(change_channels, width_channels, kernel_size=1),  # the last layer of residual block in the conv block\n","                nn.BatchNorm2d(width_channels),                                 #                -> the first layer of next residual block\n","                nn.ReLU(inplace=True)\n","            )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(width_channels,width_channels,kernel_size=3,stride=stride,padding=1,groups=groups),\n","            nn.BatchNorm2d(width_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.conv3 = nn.Sequential(\n","                nn.Conv2d(width_channels,change_channels,kernel_size=1),   # change_channels are input_channels->1X1 conv2_nums\n","                nn.BatchNorm2d(change_channels)\n","            )\n","        if self.downsampling:   # if down_sampling  define layer stride = 2\n","            self.downsampling_layer = nn.Sequential(\n","                nn.Conv2d(down_sampling_channels,change_channels,1,stride=2),\n","                nn.BatchNorm2d(change_channels)\n","            )\n","        # conv2 first add 1x1_conv to let dim same\n","        if first_layer:\n","            self.downsampling_layer = nn.Sequential(\n","                nn.Conv2d(in_channels,change_channels,1,stride=1),\n","                nn.BatchNorm2d(change_channels)\n","            )\n","    def forward(self,x):\n","        identity = x\n","        x_conv1 = self.conv1(x)\n","        x_conv2 = self.conv2(x_conv1)\n","        x_conv3 = self.conv3(x_conv2)\n","        if self.downsampling or self.first_layer:   # if down_sampling x should go through down_sampling layer(1x1 conv ) to let dim same\n","            identity = self.downsampling_layer(x)\n","        return F.relu(x_conv3+identity)\n","\n","# conv_block\n","def resnet_block(in_channels,num_block,down_sampling=False,change=False,groups=1,depth=1):\n","    \"\"\"\n","\n","    :param in_channels: in_channels\n","    :param num_block: num of block\n","    :param down_sampling: down_sampling or not\n","    :param change:stride from 1->2 residual block 3x3 conv\n","    :param groups:groups\n","    :param depth: depth\n","    :return:\n","    \"\"\"\n","    conv_blk = []\n","    for i in range(num_block):\n","        if i == 0 :     # first layer of block\n","            if change:\n","                conv_blk.append(Bottleneck(in_channels,first_layer=False,stride=2,down_sampling=down_sampling,next_layer=True,groups=groups,depth=depth))\n","            else:\n","                conv_blk.append(\n","                    Bottleneck(in_channels, first_layer=True, stride=1, down_sampling=down_sampling,groups=groups,depth=depth))     # conv2 block first layer\n","        else:\n","            conv_blk.append(Bottleneck(in_channels,first_layer=False,stride=1,down_sampling=False,groups=groups,depth=depth))       # the left layer\n","\n","    return nn.Sequential(*conv_blk)\n","\n","class ResNet_50(nn.Module):\n","    def __init__(self):\n","        super(ResNet_50, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=7, stride=2,padding=3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.MaxPool2d(3,2,padding=1),\n","            nn.Sequential(resnet_block(64,3,down_sampling=False))\n","        )\n","        self.conv3 = resnet_block(128,4,down_sampling=True,change=True)\n","        self.conv4 = resnet_block(256,6,down_sampling=True,change=True)\n","        self.conv5 = resnet_block(512,3,down_sampling=True,change=True)\n","        self.fc = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((1, 1)),   # global avgerage pool\n","            Flatten(),\n","            nn.Linear(512 * 4, 2),\n","            nn.Softmax(dim=1)\n","        )\n","    def forward(self,x):\n","        x_conv1 = self.conv1(x)\n","        x_conv2 = self.conv2(x_conv1)\n","        x_conv3 = self.conv3(x_conv2)\n","        x_conv4 = self.conv4(x_conv3)\n","        x_conv5 = self.conv5(x_conv4)\n","        output = self.fc(x_conv5)\n","        return output\n","\n","resnet_50 = ResNet_50().to(device)\n","# print(resnet_50)\n","\n","lr, num_epochs = 0.001, 1000\n","\n","resnet_50_loss_list = []\n","\n","resnet_50_acc_list = []\n","#\n","\n","\n","# test each conv output shape\n","def test_net(net):\n","    net_tn = net\n","    X = torch.rand((1, 3, 224, 224)).to(device)\n","    for name, layer in net_tn.named_children():\n","        X = layer(X)\n","        print(name, ' output shape:\\t', X.shape)\n","# save net,loss_list and acc_list\n","def save_model(net,loss_list,acc_list):\n","    net = net.cpu()\n","    loss_list_sm = loss_list\n","    acc_list_sm = acc_list\n","    sio.savemat('/content/drive/My Drive/QML/cv/image_classification/loss.mat', {'loss': loss_list_sm})\n","    sio.savemat('/content/drive/My Drive/QML/cv/image_classification/acc.mat', {'acc': acc_list_sm})\n","    print(\"model saving...\")\n","    torch.save(net, '/content/drive/My Drive/QML/cv/image_classification/{0}.pth'.format(net.__class__.__name__))\n","\n","def train_and_test(net,train_loader,test_loader, loss_list,lr,num_epochs,acc_list):\n","\n","    train_loader = train_loader\n","    test_loader = test_loader\n","    net = net.to(device)\n","    loss_list_ep = loss_list\n","    lr = lr\n","    num_epochs = num_epochs\n","    acc_list_ep = acc_list\n","    optimizer = optim.Adam(net.parameters(), lr=lr)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    start_epoch = -1        # checkpoint start_epoch\n","    # load checkpoint\n","    model_file = '/content/drive/My Drive/QML/cv/image_classification/checkpoint.pth'\n","    if os.path.isfile(model_file):\n","        checkpoint = torch.load(model_file)\n","        acc_cp = checkpoint['acc']\n","        start_epoch = checkpoint['epoch']\n","        net.load_state_dict(checkpoint['model'])\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","        loss_list_ep = checkpoint['loss_list']\n","        acc_list_cp = checkpoint['acc_list']\n","        print('Load checkpoint at epoch %d | acc %.2f' % (start_epoch, acc_cp))\n","\n","\n","    for epoch in range(num_epochs):\n","        if epoch <= start_epoch:\n","            continue\n","        start_time = time.time()\n","        train_loss = 0.0\n","        test_acc = 0.0\n","        train_loss_n = 0\n","        test_acc_n = 0\n","        for iter,(inputs, labels) in enumerate(train_loader):     # train\n","\n","            # inputs = inputs.to(device)\n","            inputs_img = Variable(inputs).to(device)\n","            # labels = labels.to(device)\n","            labels = Variable(labels).to(device)\n","\n","            labels_predict = net(inputs_img)\n","\n","            loss = loss_func(labels_predict, labels)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if (iter+1)%5 == 0:\n","                loss_list_ep.append(loss.item())\n","                # print('train_loss %.4f '%(loss.item()))\n","            train_loss += loss.cpu().item()\n","            train_loss_n += 1\n","\n","            # del intermediate variables/tensors to reduce memory\n","            del inputs_img\n","            del labels\n","            del labels_predict\n","            del loss\n","            # maybe helpful\n","            torch.cuda.empty_cache()\n","        train_loss /= train_loss_n\n","        # print('epoch %d | train_loss %.4f'%(epoch,train_loss))\n","        loss_list_ep.append(train_loss)\n","\n","        for inputs, labels in test_loader:        # test\n","            net.eval()  # test mode to prevent BN influence\n","            with torch.no_grad():\n","                inputs_img = Variable(inputs).to(device)\n","                labels = Variable(labels).to(device)\n","                labels_predict = net(inputs_img)\n","                test_acc += torch.sum(labels_predict.cpu().argmax(dim=1) == labels.cpu()).numpy()\n","                test_acc_n += labels.shape[0]\n","                # print('test_acc %.2f' % (test_acc / test_acc_n))\n","            net.train()  # train mode\n","        test_acc /= test_acc_n\n","        acc_list_ep.append(test_acc)\n","        print('epoch %d | train_loss %.4f | test_acc %.2f | time %.2f sec'\n","              % (epoch, train_loss, test_acc , time.time() - start_time))\n","        \n","        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'max',factor=0.1,patience=3,verbose=True)\n","        # scheduler.step(test_acc)\n","        # print(optimizer.state_dict()['param_groups'][0]['lr'])\n","\n","        # save checkpoint\n","        checkpoint = {\n","            'acc': test_acc,\n","            'epoch': epoch,\n","            'model': net.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'loss_list': loss_list_ep,\n","            'acc_list' : acc_list_ep\n","        }\n","        model_file = '/content/drive/My Drive/QML/cv/image_classification/checkpoint.pth'\n","        torch.save(checkpoint, model_file)\n","        if test_acc>=0.90:      # reach acc to save model\n","            save_model(net,loss_list_ep,acc_list_ep)\n","\n","# train_and_test(resnet_50,train_loader,test_loader,resnet_50_loss_list,lr,num_epochs,resnet_50_acc_list)\n","\n","\n","# # cost function figure\n","# data_loss = sio.loadmat('/content/drive/My Drive/QML/cv/image_classification/loss.mat')\n","# loss_list = data_loss['loss']\n","# # print(loss_list)\n","# # print(np.nonzero(loss_list))\n","# loss_plt = []\n","# print(loss_list.shape[1])\n","# for i in range(loss_list.shape[1]):\n","#     if loss_list[0][i] > 0 and (i+1)%1000==0:\n","#         loss_plt.append(loss_list[0][i])\n","# list_iter = range(len(loss_plt))\n","# def draw(num_iter,loss_list):\n","#     plt.plot(num_iter,loss_list,'-b',label='loss')\n","#     plt.xlabel('epoch')\n","#     plt.ylabel('loss')\n","#     plt.title('loss')\n","#     plt.savefig('/content/drive/My Drive/QML/cv/image_classification/loss.jpg')\n","# draw(list_iter,loss_plt)\n","\n","def draw_pic(loss_iter,loss_list,acc_iter,acc_list):\n","    plt.figure()\n","    plt.subplot(121)\n","    plt.plot(loss_iter,loss_list,'-b',label='loss')\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.title('loss')\n","    # plt.savefig('/content/drive/My Drive/QML/cv/image_classification/loss.jpg')\n","\n","    plt.subplot(122)\n","    plt.plot(acc_iter,acc_list,'-g',label='acc')\n","    plt.xlabel('epoch')\n","    plt.ylabel('acc')\n","    plt.title('acc')\n","    plt.savefig('/content/drive/My Drive/QML/cv/image_classification/acc_loss.jpg')\n","\n","\n","def analyse(model_file):\n","    loss_list_ep = []\n","    acc_list_ep = []\n","    checkpoint = torch.load(model_file)\n","    loss_list_ep = checkpoint['loss_list']\n","    acc_list_ep = checkpoint['acc_list']\n","    list_iter = range(len(loss_list_ep))\n","    acc_list_iter = range(len(acc_list_ep))\n","    draw_pic(list_iter,loss_list_ep,acc_list_iter,acc_list_ep)\n","\n","model_file = '/content/drive/My Drive/QML/cv/image_classification/checkpoint.pth'\n","analyse(model_file)\n","\n"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}